1. BPE是2015年被Sennrich等人在《Neural Machine Translation of Rare Words with Subword Units》中提出的。



下面我们每一步在整张词表中找出频率最高相邻序列，并把它合并，依次循环。

```
原始词表 {'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3, 'l o w </w>': 5}
出现最频繁的序列 ('s', 't') 9
合并最频繁的序列后的词表 {'n e w e st </w>': 6, 'l o w e r </w>': 2, 'w i d e st </w>': 3, 'l o w </w>': 5}
出现最频繁的序列 ('e', 'st') 9
合并最频繁的序列后的词表 {'l o w e r </w>': 2, 'l o w </w>': 5, 'w i d est </w>': 3, 'n e w est </w>': 6}
出现最频繁的序列 ('est', '</w>') 9
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'l o w </w>': 5}
出现最频繁的序列 ('l', 'o') 7
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'lo w </w>': 5}
出现最频繁的序列 ('lo', 'w') 7
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'low e r </w>': 2, 'n e w est</w>': 6, 'low </w>': 5}
出现最频繁的序列 ('n', 'e') 6
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'low e r </w>': 2, 'ne w est</w>': 6, 'low </w>': 5}
出现最频繁的序列 ('w', 'est</w>') 6
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'low e r </w>': 2, 'ne west</w>': 6, 'low </w>': 5}
出现最频繁的序列 ('ne', 'west</w>') 6
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'low e r </w>': 2, 'newest</w>': 6, 'low </w>': 5}
出现最频繁的序列 ('low', '</w>') 5
合并最频繁的序列后的词表 {'w i d est</w>': 3, 'low e r </w>': 2, 'newest</w>': 6, 'low</w>': 5}
出现最频繁的序列 ('i', 'd') 3
合并最频繁的序列后的词表 {'w id est</w>': 3, 'newest</w>': 6, 'low</w>': 5, 'low e r </w>': 2}
```



# N-gram模型

**什么是N-gram模型？**

如果我们有一个由 m 个字/词组成的序列（或者说一个句子），我们希望算得句子的概率，根据链式规则，可得

![image-20231214142426407](images/image-20231214142426407.png)

这个概率显然并不好算，不妨利用马尔科夫链的假设，即当前这个词仅仅跟前面几个有限的词相关。

当 n=1, 一个一元模型（unigram model)即为 ：

![img](images/v2-4e5bacfab55c6fc21990be89e3c84e14_720w.webp)

当 n=2, 一个二元模型（bigram model)即为 ：

![img](images/v2-d45c62f548c4da5d47a904a09ba20159_720w.webp)

通过我们的标准语料库，我们可以近似的计算出所有的分词之间的二元条件概率，比如任意两个词，它们的条件概率分布可以近似的表示为

![image-20231214142846436](images/image-20231214142846436.png)

利用语料库建立的统计概率，对于一个新的句子，我们就可以通过计算各种分词方法对应的联合分布概率，找到最大概率对应的分词方法，即为最优分词。

N元模型的分词方法虽然很好，但是要在实际中应用也有很多问题：

* 某些生僻词，概率为0。（这种情况我们一般会使用拉普拉斯平滑，即给它一个较小的概率值）
* 句子长，分词有很多情况，计算量也非常大，这时我们可以用下一节维特比算法来优化算法时间复杂度。



**上面只是根据分词计算出句子概率，那么如何分词？维特比算法。**

为了简化原理描述，我们本节的讨论都是以二元模型为基础。

维特比算法采用的是动态规划来解决这个最优分词问题的，动态规划要求局部路径也是最优路径的一部分，很显然我们的问题是成立的。首先我们看一个简单的分词例子："人生如梦境"。它的可能分词可以用下面的概率图表示：

对于每个词，我们都可以计算出它的频率和二元条件概率。

![img](images/1042406-20170407134342035-1966704661.png)

从后往前推，最终的分词结果为"人生/如/梦境"。

# WordPiece

最早出现在2016年谷歌的论文Google’s Neural Machine Translation system。 2018 年 BERT 中也用了。

它可以被认为是 BPE 和 Unigram 算法的折中。WordPiece 也是一种贪婪算法，它利用似然性而不是计数频率来合并每次迭代中的最佳对，但配对字符的选择是基于计数频率的。因此，在选择要配对的字符方面，它与 BPE 类似；在选择要合并的最佳对方面，它与 Unigram 类似。

BPE算法通过循环的方式不断将一些高频的pair进行合并，通过贪婪的方式，每一步都将将高频的组合进行合并。这种方法存在的一个主要问题是：一个词可能存在多种拆分方式，对于算法来说，难以评估使用那个拆分方式比较合理，可以组合的列表中的优先级无法确定，通常会直接取第一个，举个例子：

![img](images/v2-5ec8d534e62e24befd16865da662f197_r.jpg)

例如：linear = **li + near** 或者 **li + n + ea + r**，这两种拆分方法哪个好坏，无法评价。所以，比BPE使用频次进行merge更好的方法是， 在merge的时候考虑merge前后的影响到底有多大。

WordPiece选择能够提升语言模型似然概率最大的相邻子词加入词表。例如，在考虑将"e"和"s"合并的时候除了会考虑"es"的概率值，还会考虑"e"和"s"的概率值。

**似然概率怎么计算？**

假设句子$S=(t1, t2,,,tn)$，则句子S的语言模型似然值是：

![image-20231214084913459](images/image-20231214084913459.png)

假设把相邻位置的x和y两个子词进行合并，合并后产生的子词记为z，此时句子S似然值的变化可表示为：

![image-20231214084959106](images/image-20231214084959106.png)

![image-20231214195702259](images/image-20231214195702259.png)

遍历所有的字符对，合并Loss最大的字符对。

**不太好理解？**

拆分前信息熵为-log(p(tz))，拆分后为-log(p(tx)) - log(p(ty)) ，拆分后-拆分前的信息增益为-log(p(tx)) - log(p(ty)) + log(p(tz))，可以变形为上式。信息增益越大越好。



**WordPiece算法的主要步骤如下：**

1. 准备足够大的训练语料
2. 确定期望的subword词表大小
3. 将单词拆分成字符序列
4. 基于第3步数据训练语言模型
5. 从所有可能的subword单元中选择加入语言模型后能最大程度地增加训练数据概率的单元作为新的单元
6. 重复第5步直到达到第2步设定的subword词表大小或概率增量低于某一阈值

构建分词表后，编码解码步骤和BPE一样。



WordPiece有两种代码实现方式：bottom-up 和 top-down。最初的WordPiece和BPE一样基于bottom-up的，而BERT是基于 top-down。对于日语、中文和韩语，这种 top-down 的方法不起作用，因为没有明确的token units 可以开始。https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm



**WordPiece的bottom-up算法实现**

WordPiece算法分词时，会在非单词开头的字符上加上`##`前缀。如love，分词是

```
l
##o
##v
##e
```

代码示例

```
from collections import defaultdict
from typing import Dict, List, Tuple

def get_word_freqs(pre_tokenized_res: List[str]) -> Dict[str, int]:
    word_freqs = defaultdict(int)
    for word in pre_tokenized_res:
        word_freqs[word] += 1
    return word_freqs


def get_word_splits(word_freqs: Dict[str, int]) -> Dict[str, List[str]]:
    word_splits = {}
    for word in word_freqs.keys():
        word_splits[word] = [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    return word_splits


def get_base_vocab(word_splits: Dict[str, List[str]]) -> List[str]:
    vocab = []
    for split in word_splits.values():
        vocab.extend(split)
    vocab = sorted(list(set(vocab)))
    return vocab


def get_pair_scores(
    word_freqs: Dict[str, int],
    word_splits: Dict[str, List[str]],
) -> Dict[Tuple[str, str], int]:
    pair_freqs = defaultdict(int)
    subword_freqs = defaultdict(int)

    for freq, split in zip(word_freqs.values(), word_splits.values()):
        if len(split) == 1:
            subword_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair_freqs[(split[i], split[i + 1])] += freq
            subword_freqs[split[i]] += freq
        subword_freqs[split[-1]] += freq

    pair_scores = {
        pair: freq / (subword_freqs[pair[0]] * subword_freqs[pair[1]]) \
        for pair, freq in pair_freqs.items()
    }
    return pair_scores


def merge_pair(pair, word_splits):
    a, b = pair
    for word in word_splits.keys():
        split = word_splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                new_word = a + b[2:] if b.startswith('##') else a + b
                split = split[:i] + [new_word] + split[i + 2:]
            i += 1
        word_splits[word] = split
    return word_splits


def wordpiece_train(vocab_size, word_freqs, word_splits):
    vocab = get_base_vocab(word_splits)
    while len(vocab) < vocab_size:
        pair_scores = get_pair_scores(word_freqs, word_splits)
        if not pair_scores:
            break
        max_score_pair = max(pair_scores, key=pair_scores.get)
        word_splits = merge_pair(max_score_pair, word_splits)
        a, b = max_score_pair
        new_word = a + b[2:] if b.startswith('##') else a + b
        vocab.append(new_word)
    return vocab



"""
I love you very much
I am very hungry
"""
corpus = ['I', 'love', 'you', 'very', 'much', 'I', 'am', 'very', 'hungry']
word_freqs = get_word_freqs(corpus)
print(word_freqs)
word_splits = get_word_splits(word_freqs)
print(word_splits)
vocab = wordpiece_train(30, word_freqs, word_splits)
# print(vocab)
```



# Unigram

https://blog.csdn.net/raelum/article/details/132225006

Unigram 语言建模2018在 《 Improving neural network translation models with multiple subword candidates》 中提出。与WordPiece一样，Unigram Language Model(ULM)同样使用语言模型来挑选子词。不同之处在于，BPE和WordPiece算法的词表大小都是从小到大变化，属于增量法。而Unigram Language Model则是减量法,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。UniLM算法考虑了句子的不同分词可能，因而能够输出带概率的多个子词分段。

我们接下来看看UniLM是如何操作的。

假设xi是句子的分词结果，当前分词下句子S的似然值可以表示为：

![image-20231214185027625](images/image-20231214185027625.png)

对于句子S，挑选似然值最大的作为分词结果，则可以表示为

![image-20231214185123914](images/image-20231214185123914.png)

在实际应用中，词表大小有上万个，直接罗列所有可能的分词组合不具有操作性。针对这个问题，可通过**维特比算法**来解决（请参考N-gram模型）。

**那怎么求解每个子词的概率P(xi)？**

ULM通过EM算法来估计。假设当前词表V, 则M步最大化的对象是如下似然函数：

![image-20231214185251951](images/image-20231214185251951.png)

其中，|D|是语料库中语料数量。上述公式的一个直观理解是，将语料库中所有句子的所有分词组合形成的概率相加。



但是，初始时，词表V并不存在。因而，ULM算法采用不断迭代的方法来构造词表以及求解分词概率：

```text
1.初始时，建立一个足够大的词表。一般，可用语料中的所有字符加上常见的子字符串初始化词表，也可以通过BPE算法初始化。
2.针对当前词表，用EM算法求解每个子词在语料上的概率。
3.对于每个子词，计算当该子词被从词表中移除时，总的loss降低了多少，记为该子词的loss。
4.将子词按照loss大小进行排序，丢弃一定比例loss最小的子词(比如20%)，保留下来的子词生成新的词表。这里需要注意的是，单字符不能被丢弃，这是为了避免OOV情况。
5.重复步骤2到4，直到词表大小减少到设定范围。
```

可以看出，ULM会保留那些以较高频率出现在很多句子的分词结果中的子词，因为这些子词如果被丢弃，其损失会很大。







设语料库中单词为$w_i$，分词后的单词为$w'_{i}$, 那么当该子词被从词表中移除时，整个语料库的loss可以表示为：

![image-20231214191606352](images/image-20231214191606352.png)



接下来，Unigram会为vocab中的每个子词计算一个score，这个score等于从vocab中移除该子词后loss的变化值。可以证明，这个值一定是非负的，因此score越小说明这个子词越不重要，故可以移除。

将vocab的子词按照score从小到大进行排序，并移除前10-20%，然后重新计算loss，反复执行，直到vocab大小符合要求。